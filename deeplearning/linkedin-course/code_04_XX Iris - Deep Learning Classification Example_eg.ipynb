{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9cc808",
   "metadata": {},
   "source": [
    "# Deep Learning Example - Iris (eg)\n",
    "\n",
    "This examples demonstrates the core deep learning model building concepts using the Keras library. The Iris flower dataset is used to build the model and perform classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698b32e-c821-405f-b652-d42e36d87009",
   "metadata": {},
   "source": [
    "### 5.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f4488ba-d0f4-4f03-9649-efeb018adbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/ettoregalli/Documents/SVILUPPO/spikes/venv/lib/python3.11/site-packages (0.0.post11)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Install related libraries for the course. \n",
    "#This is a common requirement for all other exampels too\n",
    "\n",
    "# !pip install pandas\n",
    "# !pip install tensorflow\n",
    "# !pip install scikit-learn\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabf059",
   "metadata": {},
   "source": [
    "### 4.2. Prepare Input Data for Deep Learning\n",
    "\n",
    "Perform the following steps for preparing data\n",
    "\n",
    "1. Load data into a pandas dataframe\n",
    "2. Convert the dataframe to a numpy array\n",
    "3. Scale the feature dataset\n",
    "4. Use one-hot-encoding for the target variable\n",
    "5. Split into training and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ee911c4-4535-4683-8b7f-393db03df45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n",
      "0            5.1         3.5          1.4         0.2     setosa\n",
      "1            4.9           3          1.4         0.2     setosa\n",
      "2            4.7         3.2          1.3         0.2     setosa\n",
      "3            4.6         3.1          1.5         0.2     setosa\n",
      "4              5         3.6          1.4         0.2     setosa\n",
      "..           ...         ...          ...         ...        ...\n",
      "145          6.7           3          5.2         2.3  virginica\n",
      "146          6.3         2.5            5         1.9  virginica\n",
      "147          6.5           3          5.2           2  virginica\n",
      "148          6.2         3.4          5.4         2.3  virginica\n",
      "149          5.9           3          5.1         1.8  virginica\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('iris.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    raw_data = [row for row in reader]\n",
    "  \n",
    "\n",
    "    df = pd.DataFrame.from_records(raw_data)\n",
    "\n",
    "    print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db4bd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** PREPARE INPUT DATA *****\n",
      "\n",
      "Loaded Data :\n",
      "------------------------------------\n",
      "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "\n",
      "Features before scaling :\n",
      "------------------------------------\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "Target before scaling :\n",
      "------------------------------------\n",
      "[0. 0. 0. 0. 0.]\n",
      "\n",
      "Features after scaling :\n",
      "------------------------------------\n",
      "[[-0.90068117  1.01900435 -1.34022653 -1.3154443 ]\n",
      " [-1.14301691 -0.13197948 -1.34022653 -1.3154443 ]\n",
      " [-1.38535265  0.32841405 -1.39706395 -1.3154443 ]\n",
      " [-1.50652052  0.09821729 -1.2833891  -1.3154443 ]\n",
      " [-1.02184904  1.24920112 -1.34022653 -1.3154443 ]]\n",
      "\n",
      "Target after one-hot-encoding :\n",
      "------------------------------------\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "Train Test Dimensions:\n",
      "------------------------------------\n",
      "(135, 4) (135, 3) (15, 4) (15, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"***** PREPARE INPUT DATA *****\")\n",
    "\n",
    "#Load Data and review content\n",
    "iris_data = pd.read_csv(\"iris.csv\")\n",
    "\n",
    "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
    "print(iris_data.head())\n",
    "\n",
    "#Use a Label encoder to convert String to numeric values \n",
    "#for the target variable\n",
    "\n",
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "iris_data['Species'] = label_encoder.fit_transform(\n",
    "                                iris_data['Species'])\n",
    "\n",
    "#Convert input to numpy array\n",
    "np_iris = iris_data.to_numpy()\n",
    "\n",
    "#Separate feature and target variables\n",
    "X_data = np_iris[:,0:4]\n",
    "Y_data=np_iris[:,4]\n",
    "\n",
    "print(\"\\nFeatures before scaling :\\n------------------------------------\")\n",
    "print(X_data[:5,:])\n",
    "print(\"\\nTarget before scaling :\\n------------------------------------\")\n",
    "print(Y_data[:5])\n",
    "\n",
    "#Create a scaler model that is fit on the input data.\n",
    "scaler = StandardScaler().fit(X_data)\n",
    "\n",
    "#Scale the numeric feature variables\n",
    "X_data = scaler.transform(X_data)\n",
    "\n",
    "#Convert target variable as a one-hot-encoding array\n",
    "Y_data = tf.keras.utils.to_categorical(Y_data,3)\n",
    "\n",
    "print(\"\\nFeatures after scaling :\\n------------------------------------\")\n",
    "print(X_data[:5,:])\n",
    "print(\"\\nTarget after one-hot-encoding :\\n------------------------------------\")\n",
    "print(Y_data[:5,:])\n",
    "\n",
    "#Split training and test data\n",
    "X_train,X_test,Y_train,Y_test = train_test_split( X_data, Y_data, test_size=0.10)\n",
    "\n",
    "print(\"\\nTrain Test Dimensions:\\n------------------------------------\")\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "print (type(X_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5fad2",
   "metadata": {},
   "source": [
    "### 4.3. Creating a Model\n",
    "\n",
    "Creating a model in Keras requires defining the following\n",
    "\n",
    "1. Number of hidden layers\n",
    "2. Number of nodes in each layer\n",
    "3. Activation functions\n",
    "4. Loss Function & Accuracy measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a0be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden-Layer-1 (Dense)      (None, 128)               640       \n",
      "                                                                 \n",
      " Hidden-Layer-2 (Dense)      (None, 128)               16512     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17539 (68.51 KB)\n",
      "Trainable params: 17539 (68.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#Number of classes in the target variable\n",
    "NB_CLASSES=3\n",
    "\n",
    "#Create a sequencial model in Keras\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Add the first hidden layer\n",
    "model.add(keras.layers.Dense(128,                    #Number of nodes\n",
    "                             input_shape=(4,),       #Number of input variables\n",
    "                              name='Hidden-Layer-1', #Logical name\n",
    "                              activation='relu'))    #activation function\n",
    "\n",
    "#Add a second hidden layer\n",
    "model.add(keras.layers.Dense(128,\n",
    "                              name='Hidden-Layer-2',\n",
    "                              activation='relu'))\n",
    "\n",
    "#Add an output layer with softmax activation\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                             name='Output-Layer',\n",
    "                             activation='softmax'))\n",
    "\n",
    "#Compile the model with loss & metrics\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Print the model meta-data\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6677e",
   "metadata": {},
   "source": [
    "### 4.4. Training and evaluating the Model\n",
    "\n",
    "Training the model involves defining various training models and then perform \n",
    "forward and back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a9ddba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Progress:\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining Progress:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#Fit the model. This will perform the entire training cycle, including\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#forward propagation, loss computation, backward propagation and gradient descent.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#Execute for the specified batch sizes and epoch\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Perform validation after each epoch \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m history\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m,\n\u001b[1;32m     21\u001b[0m           Y_train,\n\u001b[1;32m     22\u001b[0m           batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     23\u001b[0m           epochs\u001b[38;5;241m=\u001b[39mEPOCHS,\n\u001b[1;32m     24\u001b[0m           verbose\u001b[38;5;241m=\u001b[39mVERBOSE,\n\u001b[1;32m     25\u001b[0m           validation_split\u001b[38;5;241m=\u001b[39mVALIDATION_SPLIT)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAccuracy during Training :\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Make it verbose so we can see the progress\n",
    "VERBOSE=1\n",
    "\n",
    "#Setup Hyper Parameters for training\n",
    "\n",
    "#Set Batch size\n",
    "BATCH_SIZE=16\n",
    "#Set number of epochs\n",
    "EPOCHS=10\n",
    "#Set validation split. 20% of the training data will be used for validation\n",
    "#after each epoch\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
    "\n",
    "#Fit the model. This will perform the entire training cycle, including\n",
    "#forward propagation, loss computation, backward propagation and gradient descent.\n",
    "#Execute for the specified batch sizes and epoch\n",
    "#Perform validation after each epoch \n",
    "history=model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "print(\"\\nAccuracy during Training :\\n------------------------------------\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Plot accuracy of the model after each epoch.\n",
    "pd.DataFrame(history.history)[\"accuracy\"].plot(figsize=(8, 5))\n",
    "plt.title(\"Accuracy improvements with Epoch\")\n",
    "plt.show()\n",
    "\n",
    "#Evaluate the model against the test dataset and print results\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55efdff7",
   "metadata": {},
   "source": [
    "### 4.5. Saving and Loading Models\n",
    "\n",
    "The training and inference environments are usually separate. Models need to be saved after they are validated. They are then loaded into the inference environments for actual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7434d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`Hidden-Layer-1_input` is not a valid tf.function parameter name. Sanitizing to `Hidden_Layer_1_input`.\n",
      "WARNING:absl:`Hidden-Layer-1_input` is not a valid tf.function parameter name. Sanitizing to `Hidden_Layer_1_input`.\n",
      "WARNING:absl:`Hidden-Layer-1_input` is not a valid tf.function parameter name. Sanitizing to `Hidden_Layer_1_input`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: iris_save/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: iris_save/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Hidden-Layer-1 (Dense)      (None, 128)               640       \n",
      "                                                                 \n",
      " Hidden-Layer-2 (Dense)      (None, 128)               16512     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 3)                 387       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17539 (68.51 KB)\n",
      "Trainable params: 17539 (68.51 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Saving a model\n",
    "    \n",
    "model.save(\"iris_save\")\n",
    "    \n",
    "#Loading a Model \n",
    "loaded_model = keras.models.load_model(\"iris_save\")\n",
    "\n",
    "#Print Model Summary\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc6fb5",
   "metadata": {},
   "source": [
    "### 4.6. Predictions with Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58037d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Prediction Output (Probabilities) : [[0.02282826 0.6545039  0.32266787]]\n",
      "Prediction is  ['versicolor']\n"
     ]
    }
   ],
   "source": [
    "#Raw prediction data\n",
    "prediction_input = [[6.6, 3. , 4.4, 1.4]]\n",
    "\n",
    "#Scale prediction data with the same scaling model\n",
    "scaled_input = scaler.transform(prediction_input)\n",
    "\n",
    "#Get raw prediction probabilities\n",
    "raw_prediction = model.predict(scaled_input)\n",
    "print(\"Raw Prediction Output (Probabilities) :\" , raw_prediction)\n",
    "\n",
    "#Find prediction\n",
    "prediction = np.argmax(raw_prediction)\n",
    "print(\"Prediction is \", label_encoder.inverse_transform([prediction]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc76d3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
